{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1199173a-309f-4acc-bed6-f6ed8811354e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Project Details\n",
    "\n",
    "Our world is facing many urgent challenges, such as climate change, water insecurity, and food insecurity. Maintaining and improving quality of life around the world requires bringing together innovators across disciplines and countries to find creative solutions.\n",
    "\n",
    "One critical tool for understanding and improving the urgent challenges facing our world is Earth observation data, meaning data that is gathered in outer space about life here on Earth! Earth observation data provides accurate and publicly accessible information on our atmosphere, oceans, ecosystems, land cover, and built environment. The United States and its partners have a long history of exploring outer space and making satellite, airborne, and in-situ sensor datasets openly available to all.\n",
    "\n",
    "Your goal in this challenge is to create a visualization using Earth observation data that advances at least one of the following Sustainable Development Goals (SDGs):\n",
    "\n",
    "- 6: Clean Water and Sanitation\n",
    "\n",
    "By participating, you can be part of NASA's initiative to Transform to Open Science and to make Earth observation data available to all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92387ad-8f90-400a-812a-184314c9deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating System and File Handling\n",
    "import os\n",
    "import zipfile\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "# Data Manipulation and Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "# Visualization\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import plot\n",
    "\n",
    "# Web Scraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Geometry and Spatial Analysis\n",
    "from shapely.geometry import Point, Polygon, MultiPolygon\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Image and Raster Data Processing\n",
    "from io import BytesIO\n",
    "import rasterio\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb13e8-6584-404c-b5c9-7e27af19753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_geotiff_from_zip(zip_file_path, name_contains):\n",
    "    # Open the provided zip file\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n",
    "        # Get a list of files in the zip archive that match the provided pattern\n",
    "        matching_files = [file for file in zip_file.namelist() if name_contains in file]\n",
    "\n",
    "        # Raise an error if no matching files are found\n",
    "        if not matching_files:\n",
    "            raise ValueError(f\"No files in the zip archive match the pattern: {name_contains}\")\n",
    "\n",
    "        # Select the first matching file\n",
    "        selected_file_name = matching_files[0]\n",
    "\n",
    "        # Open the selected file within the zip archive\n",
    "        with zip_file.open(selected_file_name) as file:\n",
    "            # Check if the file is compressed and handle accordingly\n",
    "            if selected_file_name.lower().endswith('.gz'):\n",
    "                with gzip.GzipFile(fileobj=file) as gzipped_file:\n",
    "                    # Read the content of the gzipped file and store it in a BytesIO object\n",
    "                    geotiff_content = BytesIO(gzipped_file.read())\n",
    "            else:\n",
    "                # If the file is not compressed, read its content directly into a BytesIO object\n",
    "                geotiff_content = BytesIO(file.read())\n",
    "\n",
    "    # Open the GeoTIFF dataset using rasterio\n",
    "    dataset = rasterio.open(geotiff_content)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c504263-d4e7-4e11-a9b9-931d0c9c1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_water_indices(image_path):\n",
    "    # Sentinel-2 L2A bands\n",
    "    red_band = open_geotiff_from_zip(image_path, 'B04')  # Red, 664.6 nm (S2A), 665.0 nm (S2B)    \n",
    "    green_band = open_geotiff_from_zip(image_path, 'B03')  # Green, 559.8 nm (S2A), 559.0 nm (S2B)    \n",
    "    blue_band = open_geotiff_from_zip(image_path, 'B02')  # Blue, 492.4 nm (S2A), 492.1 nm (S2B)    \n",
    "    nir_band = open_geotiff_from_zip(image_path, 'B08')  # NIR, 832.8 nm (S2A), 833.0 nm (S2B)    \n",
    "    swir_band = open_geotiff_from_zip(image_path, 'B11')  # SWIR, 1613.7 nm (S2A), 1610.4 nm (S2B)\n",
    "    swir_band_ = open_geotiff_from_zip(image_path, 'B12')  # SWIR, 2202.4 nm (S2A), 2185.7 nm (S2B)    \n",
    "    water_vapour = open_geotiff_from_zip(image_path, 'B09')  # Water vapour, 945.1 nm (S2A), 943.2 nm (S2B)\n",
    "    true_color = open_geotiff_from_zip(image_path, 'True_color')  # RGB Sat Image\n",
    "\n",
    "    # Read band data as arrays\n",
    "    red_data = red_band.read(1)\n",
    "    green_data = green_band.read(1)\n",
    "    blue_data = blue_band.read(1)\n",
    "    nir_data = nir_band.read(1)\n",
    "    swir_data = swir_band.read(1)\n",
    "\n",
    "    # Read the true-color RGB bands\n",
    "    true_color_ = true_color.read([1, 2, 3])\n",
    "    # Stack the bands to create an RGB image\n",
    "    true_color_satimage = true_color_.transpose(1, 2, 0)\n",
    "    \n",
    "    # Detect RGB channels\n",
    "    band_descriptions = true_color.descriptions\n",
    "    print('band_descriptions: ', band_descriptions)\n",
    "\n",
    "    # Calculate Normalized Difference Water Index (NDWI) for water extent\n",
    "    \"\"\"\n",
    "    NDWI (Normalized Difference Water Index):\n",
    "        Measures water content.\n",
    "        Normal: 0 to 0.2 (clear water).\n",
    "        Dangerous: > 0.4 (increased turbidity).\n",
    "    \"\"\"\n",
    "    ndwi = (green_data - nir_data) / (green_data + nir_data)\n",
    "    # Apply NDWI threshold to identify water pixels\n",
    "    water_mask = np.where(ndwi > 0.5, 1, 0)\n",
    "\n",
    "    # Calculate the Normalized Water Index (NWI)\n",
    "    nwi = (nir_data - swir_data) / (nir_data + swir_data)\n",
    "    # Thresholding to identify water pixels\n",
    "    threshold_w=0.4\n",
    "    water_mask = np.where(nwi < threshold_w, 1, 0)\n",
    "    # Get the pixel resolution\n",
    "    transform = true_color.transform\n",
    "    pixel_area = abs(transform[0]) * abs(transform[4])\n",
    "    \n",
    "    # Calculate Normalized Difference Snow Index (NDSI) for snow detection\n",
    "    \"\"\"\n",
    "    NDSI (Normalized Difference Snow Index):\n",
    "        Identifies snow cover.\n",
    "        Normal: 0 to 0.2 (minimal snow).\n",
    "        Dangerous: > 0.4 (extensive snow).\n",
    "    \"\"\"\n",
    "    ndsi = (green_data - swir_data) / (green_data + swir_data)\n",
    "\n",
    "    # Calculate Turbidity Index (e.g., Red-NIR or other suitable indices)\n",
    "    \"\"\"\n",
    "    Turbidity Index:\n",
    "        Gauges water clarity.\n",
    "        Normal: 0 to 5 NTU (clear water).\n",
    "        Dangerous: > 5 NTU (cloudy water).\n",
    "    \"\"\"\n",
    "    turbidity_index = red_data / nir_data  # Placeholder, adjust as needed\n",
    "\n",
    "    # Calculate Chlorophyll-a concentration using a suitable algorithm\n",
    "    \"\"\"\n",
    "    Chlorophyll Concentration:\n",
    "        Estimates chlorophyll levels.\n",
    "        Normal: 0 to 10 µg/L (healthy ecosystems).\n",
    "        Dangerous: > 20 µg/L (excessive algal growth).\n",
    "        Unit: Micrograms per liter (µg/L)\n",
    "    \"\"\"\n",
    "    # chlorophyll_concentration = 10 * (nir_data - red_data) / (nir_data + red_data)  # Placeholder, adjust as needed\n",
    "    chlorophyll_concentration = np.where(water_mask, 10 * (nir_data - red_data) / (nir_data + red_data), 0)\n",
    "    \n",
    "    # Calculate Evapotranspiration Index (e.g., NDVI or other suitable indices)\n",
    "    \"\"\"\n",
    "    Evapotranspiration Index:\n",
    "        Reflects water loss from soil and plants.\n",
    "        Normal: Varied based on vegetation health / 0 to 0.2 (Low vegetation cover, limited water loss)\n",
    "        Moderate: 0.2 to 0.5 (Moderate vegetation cover, moderate water loss)\n",
    "        Dangerous: 0.5 to 1 (High vegetation cover, significant water loss)\n",
    "    \"\"\"\n",
    "    evapotranspiration_index = (nir_data - red_data) / (nir_data + red_data)  # Placeholder, adjust as needed\n",
    "\n",
    "    # Calculate the Harmful Algal Bloom (HAB) Index based on Sentinel-2 satellite data.\n",
    "    \"\"\"\n",
    "     Algal Bloom (AB) Index:\n",
    "        Reflects the potential presence of harmful algal blooms in water bodies.\n",
    "        Normal: Low likelihood of HABs / 0 to 0.1 (Low chlorophyll-a concentration)\n",
    "        Moderate: 0.1 to 0.3 (Moderate chlorophyll-a concentration, increased risk)\n",
    "        Dangerous: 0.3 to 1 (High chlorophyll-a concentration, significant risk of HABs)\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate Algae Index (AI)\n",
    "    algae_index = (1.5 * red_data - 1.3 * nir_data) / (red_data + 1.4 * blue_data - 0.6 * green_data + 0.04)\n",
    "\n",
    "    # Create a DataFrame with median values\n",
    "    median_values = {\n",
    "        'NDWI': np.nanmedian(ndwi),\n",
    "        'NDSI': np.nanmedian(ndsi),\n",
    "        'Turbidity_Index': np.nanmedian(turbidity_index),\n",
    "        'Chlorophyll_Concentration': np.nanmedian(chlorophyll_concentration),\n",
    "        'Evapotranspiration_Index': np.nanmedian(evapotranspiration_index),\n",
    "        'true_color_satimage': np.nanmean(true_color_satimage)\n",
    "    }\n",
    "    df_median_values = pd.DataFrame(median_values, index=[0])\n",
    "    \n",
    "    ndwi[np.isnan(ndwi)] = df_median_values.NDWI\n",
    "    ndsi[np.isnan(ndsi)] = df_median_values.NDSI\n",
    "    turbidity_index[np.isnan(turbidity_index)] = df_median_values.Turbidity_Index\n",
    "    chlorophyll_concentration[np.isnan(chlorophyll_concentration)] = df_median_values.Chlorophyll_Concentration\n",
    "    evapotranspiration_index[np.isnan(evapotranspiration_index)] = df_median_values.Evapotranspiration_Index\n",
    "    true_color_satimage[np.isnan(true_color_satimage)] = df_median_values.true_color_satimage\n",
    "\n",
    "    return  ndwi, ndsi, turbidity_index, chlorophyll_concentration, evapotranspiration_index, df_median_values, true_color_satimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa0841-e987-46d6-8e15-69ad2a8266ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_indices(ndwi, turbidity_index, chlorophyll_concentration, evapotranspiration_index, true_color_satimage, title, fname, output='.'):\n",
    "    # Create a figure with a specified size\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    # Subplot 1: Normalized Difference Water Index (Water Extent)\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.imshow(ndwi, cmap='Blues')\n",
    "    plt.colorbar(label='NDWI')  # Add colorbar with label\n",
    "    plt.title('Normalized Difference Water Index (Water Extent)')\n",
    "\n",
    "    # Subplot 2: Normalized Difference Snow Index (snow detection)\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.imshow(ndwi, cmap='plasma')\n",
    "    plt.colorbar(label='NDSI')  # Add colorbar with label\n",
    "    plt.title('Normalized Difference Snow Index (snow detection)')\n",
    "\n",
    "    # Subplot 3: Turbidity Index\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.imshow(turbidity_index, cmap='viridis')\n",
    "    plt.colorbar(label='Turbidity Index')  # Add colorbar with label\n",
    "    plt.title('Turbidity Index')\n",
    "\n",
    "    # Subplot 4: Chlorophyll Concentration\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.imshow(chlorophyll_concentration, cmap='viridis')\n",
    "    plt.colorbar(label='Chlorophyll Concentration')  # Add colorbar with label\n",
    "    plt.title('Chlorophyll Concentration')\n",
    "\n",
    "    # Subplot 5: Evapotranspiration Index\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.imshow(evapotranspiration_index, cmap='RdYlGn')\n",
    "    plt.colorbar(label='Evapotranspiration Index')  # Add colorbar with label\n",
    "    plt.title('Evapotranspiration Index')\n",
    "\n",
    "    # Subplot 6: True Color Satellite Image\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.imshow(true_color_satimage)\n",
    "    plt.colorbar(label='True Color Satellite Image')  # Add colorbar with label\n",
    "    plt.title('Satellite Image')\n",
    "\n",
    "    # Add a big title to the entire set of subplots\n",
    "    plt.suptitle('Water Analysis and Quality Indices: ' + title, fontsize=14, y=0.97)    \n",
    "\n",
    "    # Save the figure as an image in the specified output directory\n",
    "    plt.savefig(f'{output}/GIS_Analysis_Plot/{fname}.png')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ce7fc-8b18-4dd7-913b-956a0ced2a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_water(image_path):\n",
    "    # Sentinel-2 L2A bands\n",
    "    green_band = open_geotiff_from_zip(image_path, 'B03')  # Green, 559.8 nm (S2A), 559.0 nm (S2B)\t\n",
    "    nir_band = open_geotiff_from_zip(image_path, 'B08')  # NIR, 832.8 nm (S2A), 833.0 nm (S2B)\t\n",
    "    \n",
    "    # Extract bands for green and NIR (Near Infrared)\n",
    "    green_data = green_band.read(1)\n",
    "    nir_data = nir_band.read(1)\n",
    "\n",
    "    # Calculate Normalized Difference Water Index (NDWI)\n",
    "    ndwi = (green_data - nir_data) / (green_data + nir_data)\n",
    "\n",
    "    # Thresholding to identify water pixels\n",
    "    water_mask = ndwi >= 0.1  # Adjust the threshold as needed\n",
    "    print()\n",
    "    return water_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5f260-31e7-402f-9e8a-73f43130fa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the source directory\n",
    "src_dir_ = '.'\n",
    "# Set the CDS API configuration file path\n",
    "os.environ['CDSAPI_RC'] = f'{src_dir_}/.cdsapirc'\n",
    "\n",
    "# Function to download weather data to DataFrame in parallel\n",
    "def download_weather_data_to_dataframe_parallel(data):\n",
    "    global src_dir_\n",
    "\n",
    "    # List of weather variables to download\n",
    "    variables = ['2m_temperature', 'total_precipitation', 'snowfall', 'wind_speed', 'mean_sea_level_pressure',\n",
    "                 'relative_humidity', 'specific_humidity', 'total_cloud_cover', 'high_vegetation_cover',\n",
    "                 'lake_total_layer_temperature', 'carbon_monoxide']\n",
    "\n",
    "    # CDS API source and product type\n",
    "    src = 'reanalysis-era5-single-levels'\n",
    "    product_type = 'reanalysis'\n",
    "\n",
    "    # Path to the zip file to store downloaded data\n",
    "    zip_file_path = f'{src_dir_}/weather_data.zip'\n",
    "\n",
    "    # Extract relevant information from the input data\n",
    "    year, month, day, polygon, site_id = data['year'].values[0], data['month'].values[0], data['day'].values[0], data['geometry'].values[0], data['site_id'].values[0]\n",
    "\n",
    "    # Adjusted temporary file name for storing the downloaded data\n",
    "    temp_file_name = f'{src_dir_}/{site_id}_{year}_{month}_{day}.nc'\n",
    "\n",
    "    # Check if the specified year is earlier than 1940, return an empty DataFrame\n",
    "    if int(year) < 1940:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Use a zipfile to add downloaded files to a zip archive\n",
    "    with zipfile.ZipFile(zip_file_path, 'a') as z:\n",
    "        if os.path.basename(temp_file_name) not in z.namelist():\n",
    "            # Calculate bounding box\n",
    "            minx, miny, maxx, maxy = polygon\n",
    "            area = [maxy, minx, miny, maxx]  # North, West, South, East\n",
    "            # Initialize CDS API client\n",
    "            c = cdsapi.Client(timeout=160, quiet=True, debug=False)\n",
    "\n",
    "            try:\n",
    "                # Download data from CDS API\n",
    "                c.retrieve(\n",
    "                    src,\n",
    "                    {\n",
    "                        'product_type': product_type,\n",
    "                        'format': 'netcdf',\n",
    "                        'variable': variables,\n",
    "                        'year': year,\n",
    "                        'month': month,\n",
    "                        'day': day,\n",
    "                        'time': ['00:00', '01:00', '02:00', '03:00', '04:00', '05:00', '06:00', '07:00', '08:00',\n",
    "                                 '09:00', '10:00', '11:00', '12:00', '13:00', '14:00', '15:00', '16:00', '17:00',\n",
    "                                 '18:00', '19:00', '20:00', '21:00', '22:00', '23:00'],\n",
    "                        'area': area,\n",
    "                    },\n",
    "                    temp_file_name)\n",
    "\n",
    "                # Add only the file to the main zip (not the entire path)\n",
    "                z.write(temp_file_name, os.path.basename(temp_file_name))\n",
    "                os.remove(temp_file_name)  # Remove the temporary file\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in data retrieval: {e}\")\n",
    "                year -= 1\n",
    "\n",
    "    # Adjust the temporary file name\n",
    "    temp_file_name = f'{src_dir_}/{site_id}_{year}_{month}_{day}.nc'\n",
    "\n",
    "    # Read the file from the main zip\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as z:\n",
    "        if os.path.basename(temp_file_name) not in z.namelist():\n",
    "            all_files = z.namelist()\n",
    "            # Filter files to select only those ending with '.nc'\n",
    "            nc_files = [file for file in all_files if file.startswith(f'{site_id}_{year}')]\n",
    "            nc_files = [file for file in all_files if file.endswith(f'{year}_{month}_{day}_.nc')] if len(\n",
    "                nc_files) == 0 else nc_files\n",
    "            # Select a random file from the list\n",
    "            random_file = random.choice(nc_files)\n",
    "            # Adjust temp_file_name to the randomly selected file\n",
    "            temp_file_name = random_file\n",
    "\n",
    "        print('temp_file_name: ', temp_file_name)\n",
    "        with z.open(os.path.basename(temp_file_name)) as f:\n",
    "            with xr.open_dataset(f) as ds:\n",
    "                # Convert to DataFrame\n",
    "                df = ds.to_dataframe().reset_index()\n",
    "\n",
    "                if df.empty:\n",
    "                    raise ValueError(\"No data available for the specified date and area.\")\n",
    "\n",
    "                # Calculate statistics for each variable (min, max, mean)\n",
    "                stats = df.describe().loc[['min', 'max', 'mean']]\n",
    "                stats.drop(columns=['longitude', 'latitude'], inplace=True)\n",
    "\n",
    "                # Create a DataFrame with statistics\n",
    "                stats_df = stats.unstack().to_frame().transpose()\n",
    "                stats_df.columns = [f'{var}_{stat}' for stat, var in stats_df.columns]\n",
    "                stats_df['site_id'] = site_id\n",
    "                stats_df['year'] = year\n",
    "                stats_df['month'] = month\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e5225-d536-407e-a297-ae81cea173be",
   "metadata": {},
   "source": [
    "## Ganges_river Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55ff287-c3a8-4121-a05a-e234e0b0e8ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the working directory for the Ganges River India\n",
    "working_directory = './GIS/'\n",
    "delivrables = 'deliverables/Ganges'\n",
    "\n",
    "ganges_river_images = glob.glob(f'{working_directory}Rasters/Ganges_River/*.zip')\n",
    "ganges_river_shape_fname = glob.glob(f'{working_directory}Shape_File/Ganges_River/*poly.shp')\n",
    "ganges_river_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eae726-0110-4cff-abc3-88053c02cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store shapes, area styles, and polygons\n",
    "shapes = []\n",
    "areastyle = []\n",
    "poly = []\n",
    "\n",
    "# Iterate over each shapefile in the list of Ganges river shapefiles\n",
    "for file in ganges_river_shape_fname:\n",
    "    # Read the shapefile and append it to the 'shapes' list\n",
    "    shapes.append(gpd.read_file(file))\n",
    "    # Define the area style for the shape (fill color and border color)\n",
    "    areastyle.append({'fillColor': '#0BEE00', 'color': '#0BEE00'})\n",
    "\n",
    "# Extract the first geometry from the first shape in the list and create a Polygon\n",
    "poly = Polygon(shapes[0].geometry[0])\n",
    "# Get the bounding box of the polygon's geometry\n",
    "poly_geom = poly.bounds\n",
    "# Get the centroid of the polygon\n",
    "polycenter = poly.centroid\n",
    "\n",
    "# Create a folium map centered at the polygon's centroid with a zoom level of 10\n",
    "m = folium.Map([polycenter.y, polycenter.x], zoom_start=10)\n",
    "\n",
    "# Iterate over each shape in the list and add it to the folium map\n",
    "for shape in shapes:\n",
    "    # Add GeoJSON layer to the map with the specified area style\n",
    "    folium.GeoJson(shape, style_function=lambda x: areastyle[0]).add_to(m)\n",
    "\n",
    "# Print the bounding box of the area covered by the map\n",
    "print('area bounds : ', m.get_bounds())\n",
    "# Get and set the map bounds to fit the area covered by the shapes\n",
    "project_polygon_bounds = m.get_bounds()\n",
    "m.fit_bounds(m.get_bounds())\n",
    "\n",
    "# Display the folium map\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443c833-40e2-4bae-af22-f8d6670690a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ganges_water_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over Ganges river images\n",
    "for images in tqdm(ganges_river_images):\n",
    "    # Extract date information from the image filename\n",
    "    f_name = os.path.basename(images)\n",
    "    f_name = f_name.split('.')\n",
    "    f_name_ = f_name[0][-8:len(f_name[0])]\n",
    "    f_name = f_name[0][-6:len(f_name[0])]\n",
    "    \n",
    "    print(f'India_Ganges River Water Quality Analysis {f_name_}')\n",
    "    \n",
    "    # Detect water using a custom function\n",
    "    water_mask = detect_water(images)    \n",
    "    \n",
    "    # Calculate water indices using a custom function\n",
    "    ndwi, ndsi, turbidity_index, chlorophyll_concentration, evapotranspiration_index, ganges_river_2021_median_values_df, true_color_satimage = calculate_water_indices(images)\n",
    "    \n",
    "    # Add additional metadata to the DataFrame\n",
    "    ganges_river_2021_median_values_df['Year'] = f_name[-4:len(f_name)]\n",
    "    ganges_river_2021_median_values_df['Month'] = f_name[:2]\n",
    "    ganges_river_2021_median_values_df['Day'] = f_name_[:2]\n",
    "    ganges_river_2021_median_values_df['Location'] = 'Ganges_River_India'\n",
    "    \n",
    "    # Concatenate the calculated values to the main DataFrame\n",
    "    ganges_water_df = pd.concat([ganges_water_df, ganges_river_2021_median_values_df], ignore_index=True)\n",
    "    \n",
    "    # Display the calculated water quality data\n",
    "    display(ganges_river_2021_median_values_df)\n",
    "    \n",
    "    # Plot the water indices using a custom function\n",
    "    plot_title = f_name_\n",
    "    plot_indices(ndwi, turbidity_index, chlorophyll_concentration, evapotranspiration_index, true_color_satimage, plot_title + f\"\\n\", plot_title, delivrables)\n",
    "    \n",
    "    # Create a dictionary containing weather data information\n",
    "    weather_dict = {\n",
    "        'year': f_name[-4:len(f_name)],\n",
    "        'month': f_name[:2],\n",
    "        'day': f_name_[:2],\n",
    "        'geometry': [poly_geom],\n",
    "        'site_id': 'Ganges_szone1'\n",
    "    }\n",
    "\n",
    "    # Download weather data and convert it to a DataFrame\n",
    "    weather_data = pd.DataFrame(weather_dict)\n",
    "    weather_df = download_weather_data_to_dataframe_parallel(weather_data)\n",
    "    \n",
    "    # Convert temperature from Kelvin to Celsius\n",
    "    weather_df['t2m_C'] = weather_df['t2m'] - 273.15\n",
    "    column_t2m_C = weather_df.pop('t2m_C')\n",
    "    index_after = weather_df.columns.get_loc('t2m') + 1\n",
    "    weather_df.insert(index_after, 't2m_C', column_t2m_C)\n",
    "\n",
    "    # Standardize column names in the weather data\n",
    "    std_cols = {\n",
    "        't2m': '2m_temperature(K)',\n",
    "        't2m_C': '2m_temperature(°C)',\n",
    "        'tp': 'total_precipitation(m)',\n",
    "        'sf': 'snowfall(m)',\n",
    "        'msl': 'mean_sea_level_pressure(Pa)',\n",
    "        'tcc': 'total_cloud_cover(0-1)',\n",
    "        'cvh': 'high_vegetation_cover(0-1)',\n",
    "        'ltlt': 'lake_total_layer_temperature(K)'\n",
    "    }\n",
    "\n",
    "    # Rename columns using the dictionary\n",
    "    weather_df.rename(columns=std_cols, inplace=True)\n",
    "    \n",
    "    # Convert lake_total_layer_temperature from Kelvin to Celsius\n",
    "    weather_df['lake_total_layer_temperature(°C)'] = weather_df['lake_total_layer_temperature(K)'] - 273.15\n",
    "    \n",
    "    # Convert 'time' column to datetime format\n",
    "    weather_df['time'] = pd.to_datetime(weather_df['time'])\n",
    "    # Extract the hour from the 'time' column\n",
    "    weather_df['hour'] = weather_df['time'].dt.hour\n",
    "    \n",
    "    # Convert total_cloud_cover to float32\n",
    "    weather_df['total_cloud_cover(0-1)'] = weather_df['total_cloud_cover(0-1)'].astype(np.float32)\n",
    "    \n",
    "    # Save weather data to a CSV file\n",
    "    weather_df.to_csv(f'{delivrables}/Weather_dataframes/Ganges_weather_szone1_{plot_title}.csv', index=False)\n",
    "    # Display the weather DataFrame\n",
    "    display(weather_df)\n",
    "\n",
    "# Save the Ganges water quality DataFrame to a CSV file\n",
    "ganges_water_df.to_csv(f'{delivrables}/Ganges_water_quality_szone1_{min(ganges_water_df.Year)}-{max(ganges_water_df.Year)}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6360d664-86ae-482a-b2b3-481e99745b13",
   "metadata": {},
   "source": [
    "### ganges_river data_frames Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01f5f2-8c56-43f9-84ed-b60df3be4342",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort the ganges_water_df DataFrame by the 'Year' column in descending order\n",
    "ganges_water_df = ganges_water_df.sort_values(by='Year', ascending=False)\n",
    "ganges_water_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3db05b-a8b3-43de-9b03-4e63d5ac1e20",
   "metadata": {},
   "source": [
    "## 3D interactive Turbidity_Index and Chlorophyll_Concentration Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789f1b3-d206-443d-8263-c89fa8284992",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary with water quality indices and their corresponding colors for the 3D plots\n",
    "indexes_3D_plots = {'Turbidity_Index': 'orange', 'Chlorophyll_Concentration': 'red'}\n",
    "\n",
    "# Extracting data\n",
    "x = ganges_water_df['Year'].values\n",
    "y = ganges_water_df['Month'].values\n",
    "\n",
    "# Assigning different colors for each year\n",
    "unique_years = ganges_water_df['Year'].unique()\n",
    "colors = [f'hsl({hue}, 50%, 50%)' for hue in range(0, 360, int(360 / len(unique_years)))]\n",
    "\n",
    "# Creating a dictionary to map years to colors\n",
    "year_color_mapping = dict(zip(unique_years, colors))\n",
    "\n",
    "# Iterate over each water quality index and its corresponding color\n",
    "for index_, color_ in indexes_3D_plots.items():\n",
    "    # Extract the z-axis data for the current index\n",
    "    z = ganges_water_df[index_].values\n",
    "    \n",
    "    # Create a subplot with 3D scatter plot\n",
    "    fig = make_subplots(rows=1, cols=1, specs=[[{'type': 'scatter3d'}]])\n",
    "\n",
    "    # Create a 3D scatter plot\n",
    "    # Add a scatter plot for each year\n",
    "    for year in unique_years:\n",
    "        mask = ganges_water_df['Year'] == year\n",
    "        trace = go.Scatter3d(\n",
    "            x=x[mask], y=y[mask], z=z[mask],\n",
    "            mode='markers',\n",
    "            marker=dict(color=year_color_mapping[year]),\n",
    "            name=str(year)\n",
    "        )\n",
    "        fig.add_trace(trace)\n",
    "    \n",
    "    # Update the layout of the 3D scatter plot\n",
    "    fig.update_layout(\n",
    "        scene=dict(aspectmode='cube'),\n",
    "        scene_xaxis=dict(title='Year'),\n",
    "        scene_yaxis=dict(title='Month'),\n",
    "        scene_zaxis=dict(title=index_),\n",
    "        title=f'3D Scatter Plot of {index_}',\n",
    "        legend=dict(title='Year'),\n",
    "        showlegend=True,\n",
    "    )\n",
    "        \n",
    "    # Export the interactive visualization as an HTML file\n",
    "    plot(fig, filename=f'{delivrables}/Water_Quality_Plots/3d_{index_}_scatter.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06ed518-61a9-4abc-8dc6-5dc8b507f592",
   "metadata": {},
   "source": [
    "## WaterQuality Barplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672be43-406b-4ead-8b4d-04a363d4f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the selected columns for the pair plot\n",
    "sel_cols = [\n",
    "    'NDWI',\n",
    "    'NDSI',\n",
    "    'Turbidity_Index',\n",
    "    'Chlorophyll_Concentration',\n",
    "    'Evapotranspiration_Index',\n",
    "    'Year'\n",
    "]\n",
    "\n",
    "# Create a DataFrame with selected columns\n",
    "df = ganges_water_df[sel_cols].copy()\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create a pair plot with 'Year' as hue\n",
    "sns.pairplot(df, hue='Year', palette='husl', markers=\"D\")\n",
    "\n",
    "# Set the title for the pair plot\n",
    "plt.suptitle(\"Distribution and Pair-Plot of Water Quality Variables by Year (All Data)\\n\")\n",
    "\n",
    "# Adjust layout to make more space for the title\n",
    "plt.subplots_adjust(top=0.9)\n",
    "\n",
    "# Save the figure to a PNG file\n",
    "export_plot_fn = f'{delivrables}/Water_Quality_Plots/Water_Quality_pairplot.png'\n",
    "plt.savefig(export_plot_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9965d8ac-b762-44ce-975f-114ca55892ad",
   "metadata": {},
   "source": [
    "## Weather data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee69679c-c47b-4ba7-951c-678120bf8f4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use glob to retrieve a list of file paths for CSV files in the specified directory\n",
    "weather_df_files = glob.glob(f'{delivrables}/Weather_dataframes/*.csv')\n",
    "weather_df_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b880cb6-194b-4e92-97aa-bef75f25cfae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over each weather dataframe file\n",
    "for file in tqdm(weather_df_files):\n",
    "    # Define the columns to be used for plotting\n",
    "    w_cols_ = [\n",
    "        'hour',\n",
    "        '2m_temperature(°C)',\n",
    "        'total_precipitation(m)',\n",
    "        'snowfall(m)',\n",
    "        'mean_sea_level_pressure(Pa)',\n",
    "        'high_vegetation_cover(0-1)',\n",
    "    ]\n",
    "    \n",
    "    # Read the CSV file with selected columns\n",
    "    file_name = os.path.basename(file)\n",
    "    w_df = pd.read_csv(file, usecols=w_cols_)\n",
    "    \n",
    "    # Plotting\n",
    "    # Create subplots with adjusted layout\n",
    "    fig, axs = plt.subplots(w_df.shape[1]-1, 1, figsize=(14, 2 + 2 * (w_df.shape[1]-1)), sharex=True, gridspec_kw={'height_ratios': [2] * (w_df.shape[1]-1)})\n",
    "\n",
    "    # Plot each column using seaborn\n",
    "    for i, col in enumerate(w_df.columns[:-1]):\n",
    "        ax = axs[i]\n",
    "        sns.lineplot(data=w_df, x='hour', y=col, ax=ax, label=col, marker='o')\n",
    "\n",
    "        # Alternate y-axis position for each subplot\n",
    "        if i % 2 == 0:\n",
    "            ax.yaxis.tick_left()\n",
    "            ax.yaxis.set_label_position(\"left\")\n",
    "        else:\n",
    "            ax.yaxis.tick_right()\n",
    "            ax.yaxis.set_label_position(\"right\")\n",
    "\n",
    "        ax.set_ylabel(col)\n",
    "    \n",
    "    # Set common labels and title\n",
    "    plt.xlabel('Hour')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.suptitle(f'Weather Data {file_name.split(\".\")[0]} (24 hours)')\n",
    "    \n",
    "    # Show legend\n",
    "    plt.legend()\n",
    "\n",
    "    # Save the figure to a PNG file\n",
    "    plt.savefig(f'{delivrables}/Weather_Quality_Plots/{file_name.split(\".\")[0].split(\"_\")[-1]}.png')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b8884-72b6-45a0-9321-09001562a4e4",
   "metadata": {},
   "source": [
    "## Dashboard Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be47e9-0b9b-404f-b425-b3178c8e821c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths for the HTML visualizations\n",
    "viz_turbidity_index = f'{delivrables}/Water_Quality_Plots/3d_Turbidity_Index_scatter.html'\n",
    "viz_chlorophyll_concentration = f'{delivrables}/Water_Quality_Plots/3d_Chlorophyll_Concentration_scatter.html'\n",
    "\n",
    "# Get data from 3D Turbidity Index visualization\n",
    "with open(viz_turbidity_index, 'r', encoding='utf-8') as viz_turbidity_report:\n",
    "    soup_turbidity = BeautifulSoup(viz_turbidity_report, 'html.parser')\n",
    "viz_turbidity_content = soup_turbidity.find_all('div')\n",
    "viz_turbidity_values = []\n",
    "\n",
    "# Get the text content between the div tags\n",
    "if viz_turbidity_content:\n",
    "    # Extract the content of each script element\n",
    "    for script in viz_turbidity_content:\n",
    "        viz_turbidity_values.append(script)\n",
    "else:\n",
    "    print(\"Turbidity scripts not found.\")\n",
    "print('Turbidity scripts:', len(viz_turbidity_values))\n",
    "\n",
    "# Get data from 3D Chlorophyll Concentration visualization\n",
    "with open(viz_chlorophyll_concentration, 'r', encoding='utf-8') as viz_chlorophyll_report:\n",
    "    soup_chlorophyll = BeautifulSoup(viz_chlorophyll_report, 'html.parser')\n",
    "viz_chlorophyll_content = soup_chlorophyll.find_all('div')\n",
    "viz_chlorophyll_values = []\n",
    "\n",
    "# Get the text content between the div tags\n",
    "if viz_chlorophyll_content:\n",
    "    # Extract the content of each script element\n",
    "    for script in viz_chlorophyll_content:\n",
    "        viz_chlorophyll_values.append(script)\n",
    "else:\n",
    "    print(\"Chlorophyll scripts not found.\")\n",
    "print('Chlorophyll scripts:', len(viz_chlorophyll_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0fae5b-8c42-48b5-a63a-02601a2e5299",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_location = 'Ganges'\n",
    "project_name = 'Water Analysis and Data Visualization of Ganges River (India)'\n",
    "output_fname = 'Water_Visualization_DashBoard.html'\n",
    "area_bounds = project_polygon_bounds  # Use snake_case for variable names\n",
    "\n",
    "data_list = glob.glob(f'{delivrables}/GIS_Analysis_Plot/*.png')\n",
    "image_names = [os.path.basename(f).split('.')[0] for f in data_list]\n",
    "\n",
    "# Define a custom sorting key function\n",
    "def custom_sort(date_string):\n",
    "    return (int(date_string[4:8]), int(date_string[2:4]), int(date_string[0:2]))\n",
    "\n",
    "# Sort the list of dates using the custom key function\n",
    "image_names_sorted = sorted(image_names, key=custom_sort)\n",
    "\n",
    "# Print the sorted list\n",
    "print('Sorted Image Names:', image_names_sorted)\n",
    "print('Number of Images:', len(image_names_sorted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36aef049-4023-4ad8-9ff4-8db315db225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## Select the DashBoard template and the output path\n",
    "viz_report_template = 'Water_Visualization_DashBoard_template.html'\n",
    "viz_report_output = f'deliverables/{output_fname}'\n",
    "shutil.copy(viz_report_template, viz_report_output)\n",
    "\n",
    "# Modify the report template for the new report\n",
    "with open(viz_report_output, 'r', encoding='utf-8') as viz_final_report:\n",
    "    soup_final = BeautifulSoup(viz_final_report, 'html.parser')\n",
    "\n",
    "# Selected main_script\n",
    "script_check = soup_final.find('script', {'id': 'main_script'})\n",
    "if script_check:\n",
    "    script_check.string = script_check.string.replace('var areaBounds = ;', f'var areaBounds = {area_bounds};')\n",
    "    script_check.string = script_check.string.replace('var imageNames = ;', f'var imageNames = {image_names_sorted};')\n",
    "    script_check.string = script_check.string.replace('var project_location = ;', f'var project_location = \"{project_location}\";')\n",
    "else:\n",
    "    print(\"Target main_script section not found in the HTML file.\")\n",
    "\n",
    "# Change the Project Title\n",
    "proj_title = soup_final.find('h1', {'id': 'project_title'})\n",
    "if proj_title:\n",
    "    proj_title.string = project_name\n",
    "else:\n",
    "    print(\"Target project_title section not found in the HTML file.\")\n",
    "    \n",
    "# Change the slider maximum steps\n",
    "slider_max = soup_final.find('input', {'id': 'image-slider'})\n",
    "if slider_max:\n",
    "    slider_max['max'] = len(image_names_sorted) - 1\n",
    "else:\n",
    "    print(\"Target image-slider section not found in the HTML file.\")\n",
    "\n",
    "# Chlorophyll_3d Interactive Plots\n",
    "Chlorophyll_3d = soup_final.find('div', {'class': 'image-canvas-3D', 'id': 'Chlorophyll_3d'})\n",
    "if Chlorophyll_3d:\n",
    "    Chlorophyll_3d.insert_after(BeautifulSoup(' '.join(str(item) for item in viz_chlorophyll_values), 'html.parser'))\n",
    "else:\n",
    "    print(\"Target Chlorophyll_3d section not found in the HTML file.\")\n",
    "\n",
    "# Turbidity_3d Interactive Plots\n",
    "Turbidity_3d = soup_final.find('div', {'class': 'image-canvas-3D', 'id': 'Turbidity_3d'})\n",
    "if Turbidity_3d:\n",
    "    Turbidity_3d.insert_after(BeautifulSoup(' '.join(str(item) for item in viz_turbidity_values), 'html.parser'))\n",
    "else:\n",
    "    print(\"Target Turbidity_3d section not found in the HTML file.\")\n",
    "\n",
    "# Save the modified HTML back to the file\n",
    "with open(viz_report_output, 'w') as file:\n",
    "    file.write(soup_final.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e79b2a-632b-4372-9f30-27bbc5017ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
